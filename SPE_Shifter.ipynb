{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib\n",
    "import analyze # takes some time since inits hax\n",
    "from channel_dict import channel_dict\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import pickle\n",
    "from scipy.stats import norm\n",
    "import shutil\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from make_runlist_new import write_spe_lists\n",
    "\n",
    "#dry run\n",
    "write_spe_lists(write=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#writes new runlists\n",
    "written=write_spe_lists(write=True)\n",
    "print(written)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This will take some time to run as job submission takes several hours. Alternatively, in the terminal you can run ./submit_jobs.sh ./runlists/[runlist name] for a single runlist, or ./large_submission.sh to submit jobs for all runlists in ./runlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def submit_job(file):\n",
    "    command = \"./submit_jobs.sh %s\" % file\n",
    "    print(command)\n",
    "    !{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in written:\n",
    "    submit_job(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acc Vs Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating acceptance and error data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import hax\n",
    "#hax already initiated when analyze is imported\n",
    "\n",
    "from spe_acceptance import data_dir_base, rawdata_dir\n",
    "\n",
    "\n",
    "def data_exists(run_number):\n",
    "    data_path=os.path.join(data_dir_base, 'run_%05d.h5' %int(run_number))\n",
    "    return os.path.exists(data_path)\n",
    "\n",
    "def all_data_exists(runlist):\n",
    "    return all([data_exists(run) for run in runlist])\n",
    "\n",
    "def file_to_list(runlist_file):\n",
    "    return [int(run) for run in runlist_file.split('.')[0].split('_')[1:4]]\n",
    "\n",
    "def get_run_time(run):\n",
    "    return hax.runs.datasets[hax.runs.datasets.number == run].start.values[0]\n",
    "    \n",
    "def find_file(run):\n",
    "    return [f for f in os.listdir('runlists') if any(int(run)==elem for elem in file_to_list(f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieves previously loaded data from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data in pickle file\n",
      "Data already exists for:  []\n"
     ]
    }
   ],
   "source": [
    "#all runlists\n",
    "runlists = [f for f in os.listdir('./runlists')]\n",
    "bottom_runs = []\n",
    "accs = []\n",
    "\n",
    "ch_acc_dict={}\n",
    "ch_err_l={}\n",
    "ch_err_u={}\n",
    "\n",
    "occ_dict={}\n",
    "\n",
    "upper_errs=[]\n",
    "lower_errs=[]\n",
    "errors = []\n",
    "\n",
    "on_ch_dict={}\n",
    "\n",
    "missing_runs = []\n",
    "LED_off = []\n",
    "resubmit_files = []\n",
    "\n",
    "with open('/project/lgrandi/xenon1t/spe_acceptance/ch_data/ch_data.pkl', 'rb') as cd:\n",
    "    if os.stat('/project/lgrandi/xenon1t/spe_acceptance/ch_data/ch_data.pkl').st_size==0:\n",
    "        print('No data in pickle file')\n",
    "        data=[]\n",
    "    else:\n",
    "        data=pickle.load(cd)\n",
    "saved_runlists=[]\n",
    "\n",
    "\n",
    "for cd in data:\n",
    "    if len(data)==0:\n",
    "        continue\n",
    "        \n",
    "    runlist = file_to_list(cd.runlist)\n",
    "    if not all_data_exists(runlist):\n",
    "        resubmit_files.append(cd.runlist)\n",
    "        for r in runs:\n",
    "            if not data_exists(r):\n",
    "                missing_runs.append(r)\n",
    "        continue    \n",
    "          \n",
    "    saved_runlists.append(cd.runlist)\n",
    "    \n",
    "    \n",
    "    acc=cd.acc\n",
    "    acc_errs_l=cd.acc_errs_l\n",
    "    acc_errs_u=cd.acc_errs_u\n",
    "    acc_stat=cd.acc_stat\n",
    "    occ=cd.occ\n",
    "    occ_stat=cd.occ_stat\n",
    "    on_channels = cd.on_channels\n",
    "    \n",
    "    on_ch_dict[cd.runlist]=on_channels\n",
    "      \n",
    "    \n",
    "    bottom_run = runlist[0]\n",
    "    topbulk_run = runlist[1]\n",
    "    topring_run = runlist[2]\n",
    "    \n",
    "    if len(on_channels) < 200:\n",
    "        LED_off.append(cd.runlist)\n",
    "    \n",
    "    else:\n",
    "        ch_acc_dict[cd.runlist]=acc\n",
    "        occ_dict[cd.runlist]=occ\n",
    "    \n",
    "        on_acc=acc[on_channels]\n",
    "\n",
    "        #make error dicts\n",
    "        ch_err_l[cd.runlist]=acc_errs_l\n",
    "        ch_err_u[cd.runlist]=acc_errs_u\n",
    "\n",
    "        bottom_runs.append(bottom_run)\n",
    "        accs.append(np.mean(on_acc))\n",
    "        #avg err=sqrt(sum(channel errors**2)/# of channels)\n",
    "        lower_errs.append(np.sqrt(np.sum(acc_errs_l[on_channels]**2)/len(on_channels)))\n",
    "        upper_errs.append(np.sqrt(np.sum(acc_errs_u[on_channels]**2)/len(on_channels)))\n",
    "\n",
    "print(\"Data already exists for: \", saved_runlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]/cvmfs/xenon.opensciencegrid.org/releases/anaconda/2.4/envs/pax_v6.8.0/lib/python3.4/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n",
      "/cvmfs/xenon.opensciencegrid.org/releases/anaconda/2.4/envs/pax_v6.8.0/lib/python3.4/site-packages/numpy/core/_methods.py:68: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "/cvmfs/xenon.opensciencegrid.org/releases/anaconda/2.4/envs/pax_v6.8.0/lib/python3.4/site-packages/numpy/core/_methods.py:82: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  warnings.warn(\"Degrees of freedom <= 0 for slice\", RuntimeWarning)\n",
      "/cvmfs/xenon.opensciencegrid.org/releases/anaconda/2.4/envs/pax_v6.8.0/lib/python3.4/site-packages/numpy/core/_methods.py:94: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/cvmfs/xenon.opensciencegrid.org/releases/anaconda/2.4/envs/pax_v6.8.0/lib/python3.4/site-packages/numpy/core/_methods.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "/home/awalsh272/SPE/SPE/analyze.py:41: RuntimeWarning: invalid value encountered in less\n",
      "  self.off_channels = np.where(self.occupancy_by_channel[0] < 0.05)[0]\n",
      "/home/awalsh272/SPE/SPE/analyze.py:108: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  corr=led_firstN / noise_firstN\n",
      "/home/awalsh272/SPE/SPE/analyze.py:108: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr=led_firstN / noise_firstN\n",
      "/home/awalsh272/SPE/SPE/analyze.py:109: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sigma_corr=corr*np.sqrt((sigma_led_firstN/led_firstN)**2 + (sigma_noise_firstN/noise_firstN)**2)\n",
      "/home/awalsh272/SPE/SPE/analyze.py:63: RuntimeWarning: divide by zero encountered in log\n",
      "  amp_occ=-np.log(amp_corr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f values:  [ 0.996  0.033  0.026  0.535  0.189  0.195  0.162  0.523  0.856  0.041\n",
      "  0.159  0.999  0.723  0.567  0.537  0.85   0.521  0.627  0.995  0.188\n",
      "  0.998  0.206  0.226  0.109  0.995  0.825  0.967  0.836  0.225  0.999\n",
      "  0.639  0.999  0.998  0.141  0.996  0.385  0.818  0.501  0.154  0.08   0.13\n",
      "  0.176  0.9    0.431  0.573  0.031  0.892  0.998  0.01   0.563  0.75   0.69\n",
      "  0.999  0.901  0.028  0.997  0.977  0.998  0.548  0.96   0.998  0.998\n",
      "  0.998  0.805  0.045  0.927  0.998  0.445  0.042  0.125  0.22   0.658\n",
      "  0.997  0.99   0.249  0.808  0.517  0.895  0.992  0.901  0.014  0.997\n",
      "  0.999  0.293  0.447  0.539  0.565  0.988  0.991  0.999  0.998  0.388\n",
      "  0.019  0.999  0.891  0.266  0.156  0.487  0.999  0.957  0.996  0.956\n",
      "  0.992  0.616  0.722  0.557  0.017  0.997  0.991  0.825  0.671  0.24\n",
      "  0.996  0.998  0.998  0.054  0.689  0.995  0.049  0.998  0.019  0.202\n",
      "  0.903  0.531  0.541  0.67   0.391  0.599  0.988  0.768  0.028  0.999\n",
      "  0.995  0.973  0.444  0.575  0.997  0.994  0.142  0.014  0.014  0.351\n",
      "  0.904  0.485  0.187  0.998  0.998  0.998  0.81   0.454  0.975  0.415\n",
      "  0.366  0.215  0.999  0.289  0.977  0.208  0.763  0.167  0.999  0.178\n",
      "  0.793  0.999  0.998  0.465  0.998  0.209  0.144  0.201  0.386  0.721\n",
      "  0.797  0.109  0.513  0.999  0.96   0.448  0.986  0.533  0.363  0.309\n",
      "  0.063  0.96   0.666  0.743  0.998  0.679  0.403  0.607  0.457  0.975\n",
      "  0.01   0.999  0.703  0.96   0.766  0.988  0.35   0.999  0.997  0.999\n",
      "  0.471  0.997  0.999  0.524  0.454  0.749  0.981  0.136  0.999  0.999\n",
      "  0.829  0.398  0.225  0.782  0.979  0.962  0.972  0.998  0.999  0.999\n",
      "  0.687  0.408  0.167  0.271  0.096  0.99   0.676  0.037  0.999  0.999\n",
      "  0.999  0.999  0.865  0.967  0.997  0.702  0.01   0.997  0.855  0.99\n",
      "  0.794  0.998  0.685  0.999  0.999  0.895]\n",
      "first median f: 0.746\n",
      "first median f shape:  ()\n",
      "median f shape:  ()\n",
      "corr thresh shape:  (1,)\n",
      "corr thresh:  [105]\n",
      "led:  (205,)\n",
      "led1N:  ()\n",
      "corr:  0.9981420985\n",
      "corr shape:  ()\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f24d4b4c6401>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_thresholds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyze\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_regular_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottom_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_stat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macceptance_3runs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottom_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopbulk_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopring_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'amplitude'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_stat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mocc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mocc_stat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moccupancy_3runs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottom_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopbulk_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopring_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'amplitude'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/awalsh272/SPE/SPE/analyze.py\u001b[0m in \u001b[0;36macceptance_3runs\u001b[0;34m(bottom_run, topbulk_run, topring_run, thresholds, space)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch_list\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel_lists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0mfrac\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mstat_errs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macceptance_fraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m         \u001b[0mret_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mch_list\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrac\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mch_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mret_stat_errs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch_list\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstat_errs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/awalsh272/SPE/SPE/analyze.py\u001b[0m in \u001b[0;36macceptance_fraction\u001b[0;34m(run_number, thresholds, space)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mbin0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bin_centers'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0mmedian_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_median_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macceptance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmedian_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbin0\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mthresholds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0macc_frac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/awalsh272/SPE/SPE/analyze.py\u001b[0m in \u001b[0;36macceptance\u001b[0;34m(self, median_f, space, errors)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0macceptance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmedian_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'amplitude'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmedian_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/awalsh272/SPE/SPE/analyze.py\u001b[0m in \u001b[0;36mresidual\u001b[0;34m(self, median_f, space)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# subtract noise spectra from LED spectra for all channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;31m# correct noise spectra by forcing sum up to val=x to be equal for both noise, led\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mcorrections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_corr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_f_correction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmedian_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LED_%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m'amplitude'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/awalsh272/SPE/SPE/analyze.py\u001b[0m in \u001b[0;36mmake_f_correction\u001b[0;34m(self, median_f, space)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'corr: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mled_firstN\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnoise_firstN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'corr shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mled_firstN\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnoise_firstN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mcorr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mled_firstN\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnoise_firstN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0msigma_corr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma_led_firstN\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mled_firstN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msigma_noise_firstN\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnoise_firstN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcorr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_corr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "#just newest runlist\n",
    "\n",
    "#newest_runlist= find_file(sorted(bottom_runs)[-1])[0]\n",
    "#print(\"Newest Runlist: \", newest_runlist)\n",
    "#runlist = file_to_list(newest_runlist)\n",
    "\n",
    "#if str(newest_runlist) not in saved_runlists:\n",
    "r_l=['runlist_18562_18563_18564.txt', 'runlist_18383_18384_18385.txt']\n",
    "rl=['./runlists/runlist_18928_18929_18930.txt', './runlists/runlist_18846_18847_18848.txt', './runlists/runlist_18562_18563_18564.txt', './runlists/runlist_18383_18384_18385.txt']\n",
    "for runlist in tqdm(r_l):\n",
    "    runs=file_to_list(runlist)\n",
    "    if not all_data_exists(runs):\n",
    "        resubmit_files.append(runlist)\n",
    "        for r in runs:\n",
    "            if not data_exists(r):\n",
    "                missing_runs.append(r)\n",
    "        continue\n",
    "\n",
    "\n",
    "    bottom_run = runs[0]\n",
    "    topbulk_run = runs[1]\n",
    "    topring_run = runs[2]\n",
    "\n",
    "    thresholds = analyze.get_thresholds(analyze.find_regular_run(bottom_run))\n",
    "    \n",
    "    acc, acc_stat = analyze.acceptance_3runs(bottom_run, topbulk_run, topring_run, thresholds, space='amplitude')\n",
    "    print(np.shape(acc), np.shape(acc_stat))\n",
    "    occ, occ_stat = analyze.occupancy_3runs(bottom_run, topbulk_run, topring_run, space='amplitude')\n",
    "\n",
    "    on_channels = np.where(occ > 0.05)[0]\n",
    "    \n",
    "    on_ch_dict[runlist]=on_channels\n",
    "\n",
    "    if len(on_channels) < 200:\n",
    "        LED_off.append(runlist)\n",
    "    \n",
    "    else:\n",
    "        on_acc = acc[on_channels]\n",
    "        on_occ = occ[on_channels]\n",
    "\n",
    "        ch_acc_dict[runlist]=acc\n",
    "        occ_dict[runlist]=occ\n",
    "\n",
    "\n",
    "        acc_errs_l = np.sqrt(acc_stat[0]**2+acc_sys**2)\n",
    "        acc_errs_u = np.sqrt(acc_stat[1]**2+acc_sys**2)\n",
    "\n",
    "        ch_err_l[runlist]=acc_errs_l\n",
    "        ch_err_u[runlist]=acc_errs_u\n",
    "\n",
    "        accs.append(np.mean(on_acc))\n",
    "        lower_errs.append(np.sqrt(np.sum(acc_errs_l[on_channels]**2)/len(on_channels)))\n",
    "        upper_errs.append(np.sqrt(np.sum(acc_errs_u[on_channels]**2)/len(on_channels)))\n",
    "        bottom_runs.append(bottom_run)\n",
    "\n",
    "        cd=analyze.ch_data(runlist, get_run_time(bottom_run), acc, acc_errs_l, acc_errs_u, acc_stat, occ, occ_stat, on_channels)\n",
    "        data.append(cd)\n",
    "\n",
    "errors=np.array([lower_errs, upper_errs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculates acceptance, errors for new runlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#just newest runlist\n",
    "\n",
    "newest_runlist= find_file(sorted(bottom_runs)[-1])[0]\n",
    "print(\"Newest Runlist: \", newest_runlist)\n",
    "runlist = file_to_list(newest_runlist)\n",
    "\n",
    "if str(newest_runlist) not in saved_runlists:\n",
    "    if not all_data_exists(runlist):\n",
    "        resubmit_files.append(newest_runlist)\n",
    "        for r in runlist:\n",
    "            if not data_exists(r):\n",
    "                missing_runs.append(r)\n",
    "            continue\n",
    "\n",
    "\n",
    "    bottom_run = runlist[0]\n",
    "    topbulk_run = runlist[1]\n",
    "    topring_run = runlist[2]\n",
    "\n",
    "    thresholds = analyze.get_thresholds(analyze.find_regular_run(bottom_run))\n",
    "\n",
    "    acc, acc_stat = analyze.acceptance_3runs(bottom_run, topbulk_run, topring_run, thresholds)\n",
    "    occ, occ_stat = analyze.occupancy_3runs(bottom_run, topbulk_run, topring_run)\n",
    "\n",
    "    on_channels = np.where(occ > 0.05)[0]\n",
    "    \n",
    "    on_ch_dict[newest_runlist]=on_channels\n",
    "\n",
    "    if len(on_channels) < 200:\n",
    "        LED_off.append(newest_runlist)\n",
    "        print(\"LED likely off for %s\" %newest_runlist)\n",
    "        \n",
    "    else:\n",
    "        on_acc = acc[on_channels]\n",
    "        on_occ = occ[on_channels]\n",
    "\n",
    "        ch_acc_dict[newest_runlist]=acc\n",
    "        occ_dict[newest_runlist]=occ\n",
    "\n",
    "        acc_errs_l = np.sqrt(acc_stat[0]**2+acc_sys**2)\n",
    "        acc_errs_u = np.sqrt(acc_stat[1]**2+acc_sys**2)\n",
    "\n",
    "        ch_err_l[newest_runlist]=acc_errs_l\n",
    "        ch_err_u[newest_runlist]=acc_errs_u\n",
    "\n",
    "        accs.append(np.mean(on_acc))\n",
    "        lower_errs.append(np.sqrt(np.sum(acc_errs_l[on_channels]**2)/len(on_channels)))\n",
    "        upper_errs.append(np.sqrt(np.sum(acc_errs_u[on_channels]**2)/len(on_channels)))\n",
    "        bottom_runs.append(bottom_run)\n",
    "\n",
    "        cd=analyze.ch_data(newest_runlist, get_run_time(bottom_run), acc, acc_errs_l, acc_errs_u, acc_stat, occ, occ_stat, on_channels)\n",
    "        data.append(cd)\n",
    "\n",
    "else:\n",
    "    print(\"Data already in pickle file\")\n",
    "\n",
    "errors=np.array([lower_errs, upper_errs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dumps all data into pickle file, prints runs with LED off and runs with missing data, deletes raw data for processed runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from spe_acceptance import change_permissions\n",
    "\n",
    "os.remove('/project/lgrandi/xenon1t/spe_acceptance/ch_data/ch_data.pkl')\n",
    "with open('/project/lgrandi/xenon1t/spe_acceptance/ch_data/ch_data.pkl', 'wb') as cd:\n",
    "    pickle.dump(data, cd)\n",
    "change_permissions('/project/lgrandi/xenon1t/spe_acceptance/ch_data/ch_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Delete raw data after processed data is in the pickle file\n",
    "from get_name import get_name\n",
    "no_data=[]\n",
    "for runlist in runlists:\n",
    "    with open(os.path.join('./runlists', runlist)) as r:\n",
    "        for run in r.readlines():\n",
    "            name=get_name(int(run))\n",
    "            raw_path=os.path.join(rawdata_dir, name)\n",
    "            pro_path=os.path.join(data_dir_base, 'run_%d.h5' % int(run))\n",
    "            if not os.path.exists(raw_path):\n",
    "                no_data.append(run)\n",
    "                continue\n",
    "            if os.path.exists(raw_path):\n",
    "                if os.path.exists(pro_path):\n",
    "                    shutil.rmtree(raw_path)\n",
    "                    print(\"Deleting raw data for: \", run, name)\n",
    "    \n",
    "print(\"These runs are missing data: \", missing_runs)\n",
    "print(\"LED likely off for these files: \", LED_off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resubmits jobs for runs missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in resubmit_files:\n",
    "    runs=file_to_list(f)\n",
    "\n",
    "    submit=os.path.join('./runlists', f)\n",
    "    if runs[0]<6731:\n",
    "        continue\n",
    "    submit_job(submit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acceptance dataframe with channels as row index, runlists as column index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc_df=pd.DataFrame(ch_acc_dict)\n",
    "channels_df=pd.DataFrame({\"channels\": [i for i in range(0,248)]})\n",
    "ch_df=pd.concat([acc_df, channels_df], axis=1)\n",
    "ch_df.set_index('channels', inplace=True)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "ch_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lower, upper errors dataframes with channels as row index, runlist as column index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lerr_df=pd.DataFrame(ch_err_l)\n",
    "ch_lerr_df=pd.concat([lerr_df, channels_df], axis=1)\n",
    "ch_lerr_df.set_index('channels', inplace=True)\n",
    "ch_lerr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uerr_df=pd.DataFrame(ch_err_u)\n",
    "ch_uerr_df=pd.concat([uerr_df, channels_df], axis=1)\n",
    "ch_uerr_df.set_index('channels', inplace=True)\n",
    "ch_lerr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "occ_df=pd.DataFrame(occ_dict)\n",
    "ch_occ_df=pd.concat([occ_df, channels_df], axis=1)\n",
    "ch_occ_df.set_index('channels', inplace=True)\n",
    "ch_occ_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rl=['runlist_18928_18929_18930.txt', 'runlist_18846_18847_18848.txt', \n",
    "    'runlist_18562_18563_18564.txt', 'runlist_18383_18384_18385.txt']\n",
    "\n",
    "with open('/project/lgrandi/xenon1t/spe_acceptance/ch_data/new_runs.txt', 'w') as nr:\n",
    "    for runlist in rl:\n",
    "        on_ch=on_ch_dict[runlist]\n",
    "        nr.write(runlist + \"\\n\")\n",
    "        nr.write(\"Mean Acc: \\n\") \n",
    "        nr.write(str(np.mean(ch_df.loc[on_ch, runlist]))+\"\\n\")\n",
    "        nr.write(\"Acc by Ch: \\n\")\n",
    "        for i in ch_df.loc[on_ch, runlist]:\n",
    "            nr.writelines(\"%s\\n\" %str(i))\n",
    "        nr.write(\"Mean Error [lower, upper]: \\n\")\n",
    "        nr.write(str(np.sqrt(np.sum(ch_lerr_df.loc[on_ch, runlist]**2))/len(on_ch))+',')\n",
    "        nr.write(str(np.sqrt(np.sum(ch_uerr_df.loc[on_ch,runlist]**2))/len(on_ch)))\n",
    "        nr.write(\"\\n Lower Error by Ch: \\n\")\n",
    "        for i in ch_lerr_df.loc[on_ch, runlist]:\n",
    "            nr.writelines(\"%s\\n\" %str(i))\n",
    "        nr.write(\"Upper Error by Ch: \\n\") \n",
    "        for i in ch_uerr_df.loc[on_ch, runlist]:\n",
    "            nr.writelines(\"%s\\n\" %str(i))\n",
    "    nr.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newest Runlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finds newest runlist, gets acceptance and occupancy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Channels that look off, note that this includes both channels that are actually off and channels that aren't performing properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#prints list of off pmts\n",
    "new_on_ch=on_ch_dict[newest_runlist]\n",
    "my_off = np.where(occ_dict[newest_runlist] < 0.05)[0]\n",
    "print(\"Channels that look off for %s: \" %newest_runlist , my_off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acceptance histogram for one runlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot the acceptance of one runlist\n",
    "\n",
    "plt.hist(ch_df.loc[new_on_ch,newest_runlist], bins=50, range=(0,1.1))\n",
    "plt.title(\"Acceptance for %s \" %newest_runlist)\n",
    "plt.xlabel(\"SPE Acceptance\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.xlim(0, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Occupancy histogram for one runlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot the occupancy of one runlist\n",
    "plt.hist(ch_occ_df.loc[new_on_ch, newest_runlist], bins=50, range=(0,1.1))\n",
    "plt.title(\"Occupancy for %s \" %newest_runlist)\n",
    "plt.xlabel(\"Occupancy\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.xlim(0,0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acceptance of all channels for the newest runlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot acceptance of each channel\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.errorbar(new_on_ch, ch_df.loc[new_on_ch,newest_runlist], yerr=[ch_lerr_df.loc[new_on_ch, newest_runlist], ch_uerr_df.loc[new_on_ch,newest_runlist]], marker='.', linestyle='none')\n",
    "plt.ylabel('Acceptance Fraction')\n",
    "plt.xlabel('Channel #')\n",
    "plt.ylim(0.5,1.2)\n",
    "plt.title('Acc by Ch for %s' %newest_runlist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prints low acceptance channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#prints list of low acc ch\n",
    "cha_dict_new={}\n",
    "\n",
    "for ch, a in zip(new_on_ch, ch_df.loc[new_on_ch, newest_runlist]):\n",
    "    cha_dict_new[a]=ch\n",
    "    \n",
    "new_low_acc_ch=[]\n",
    "for a in ch_df.loc[new_on_ch, newest_runlist]:\n",
    "    if a<0.5:\n",
    "        new_low_acc_ch.append(cha_dict_new[a])\n",
    "        \n",
    "print('Low acc ch for %s: ' %newest_runlist, new_low_acc_ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean, Median, Errors for runlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#maybe including off channels in mean\n",
    "acc_mean=np.nanmean(ch_df.loc[new_on_ch, newest_runlist])\n",
    "acc_median=np.nanmedian(ch_df.loc[new_on_ch, newest_runlist])\n",
    "lower_error=np.sqrt(np.nansum(ch_lerr_df.loc[new_on_ch, newest_runlist]**2)/len(new_on_ch))\n",
    "upper_error=np.sqrt(np.nansum(ch_uerr_df.loc[new_on_ch, newest_runlist]**2)/len(new_on_ch))\n",
    "\n",
    "print(\"Runlist: \", newest_runlist)\n",
    "print(\"Mean Acceptance: \", acc_mean)\n",
    "print(\"Median Acceptance: \", acc_median)\n",
    "print(\"Errors [l,u]: \", [lower_error, upper_error])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Acceptance per Runlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot acceptance vs time using full runlists\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "years = mdates.YearLocator()   # every year\n",
    "months = mdates.MonthLocator()  # every month\n",
    "fmt = mdates.DateFormatter('%Y-%m')\n",
    "\n",
    "bottom_runs = np.array(bottom_runs)\n",
    "dates = np.array([get_run_time(run) for run in bottom_runs])\n",
    "accs = np.array(accs)\n",
    "\n",
    "noisy_runs = [12768, 13837]\n",
    "sr1 = np.where(bottom_runs > 6731)\n",
    "sr1 = [i for i in sr1[0] if bottom_runs[i] not in noisy_runs]\n",
    "plot_dates = dates[sr1]\n",
    "plot_runs = bottom_runs[sr1]\n",
    "plot_accs = accs[sr1]\n",
    "plot_errors = [errors[0][sr1],errors[1][sr1]]\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12,8))\n",
    "ax.errorbar(plot_dates, plot_accs, yerr=plot_errors, linestyle='None', marker='.')\n",
    "ax.set_ylim(0.75, 1.0)\n",
    "plt.xticks(ax.get_xticks(), ax.get_xticklabels(), rotation=20)\n",
    "ax.xaxis.set_major_locator(months)\n",
    "ax.xaxis.set_major_formatter(fmt)\n",
    "ax.grid()\n",
    "ax.set_xlabel('date')\n",
    "ax.set_ylabel('spe acceptance')\n",
    "plt.xlim(datetime.date(year=2017, month=1, day=1), datetime.date(year=2018, month=5, day=30) )\n",
    "plt.title(\"Acceptance vs Time\")\n",
    "plt.savefig('new_moneyplot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean, Median, Errors for all runlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#add table of mean/median w errors for time evo, print days that are outliers\n",
    "\n",
    "evo_mean=np.nanmean(plot_accs)\n",
    "evo_median=np.nanmedian(plot_accs)\n",
    "evo_errs=[np.nanmean(plot_errors[0]), np.nanmean(plot_errors[1])]\n",
    "\n",
    "evo_std=np.nanstd(plot_accs)\n",
    "\n",
    "print(\"All Runlists\")\n",
    "print(\"Mean :\", evo_mean)\n",
    "print(\"Median :\", evo_median)\n",
    "print(\"Errors [l, u]: \", evo_errs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acceptance for Individual Channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots acceptances of one channel for all runlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ch=3\n",
    "\n",
    "#list of lists of runlists\n",
    "sr1_rl=[find_file(i) for i in bottom_runs[sr1]]\n",
    "\n",
    "#list of runlists\n",
    "sr1_rl=[runlist for elem in sr1_rl for runlist in elem]\n",
    "\n",
    "print(ch_lerr_df.loc[ch, sr1_rl])\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.errorbar(plot_dates, ch_df.loc[ch, sr1_rl] , yerr=[ch_lerr_df.loc[ch, sr1_rl], ch_uerr_df.loc[ch, sr1_rl]], linestyle='None', marker='.')\n",
    "plt.ylim(0.75, 1.1)\n",
    "plt.xticks(ax.get_xticks(), ax.get_xticklabels(), rotation=20)\n",
    "ax.xaxis.set_major_locator(months)\n",
    "ax.xaxis.set_major_formatter(fmt)\n",
    "plt.xlim(datetime.date(year=2017, month=1, day=1), datetime.date(year=2018, month=5, day=30) )\n",
    "plt.grid()\n",
    "plt.title(\"Channel %s\" %ch)\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('spe acceptance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#add table of mean/median w errors for time evo, print days that are outliers\n",
    "\n",
    "ch_mean=np.nanmean(ch_df.loc[ch, sr1_rl])\n",
    "ch_median=np.nanmedian(ch_df.loc[ch,sr1_rl])\n",
    "ch_errs=[np.nanmean(ch_lerr_df.loc[ch, sr1_rl]), np.nanmean(ch_uerr_df.loc[ch, sr1_rl])]\n",
    "\n",
    "ch_std=np.nanstd(ch_df.loc[ch,sr1_rl])\n",
    "\n",
    "print(\"All Runlists for ch %d\" %ch)\n",
    "print(\"Mean :\", ch_mean)\n",
    "print(\"Median :\", ch_median)\n",
    "print(\"Errors [l, u]: \", ch_errs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prints runlists with low acceptance for one channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#prints list of low acc runlists\n",
    "\n",
    "ch_accs=np.array(ch_df.loc[ch])\n",
    "\n",
    "low_acc=np.where(ch_accs<0.5)\n",
    "\n",
    "low_acc_rl=list(ch_df.columns[low_acc])\n",
    "        \n",
    "print('Low acc runlists for ch %s: ' %ch, low_acc_rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "\n",
    "months = mdates.MonthLocator()\n",
    "years = mdates.YearLocator()\n",
    "datetimefmt = mdates.DateFormatter(\"%d-%m\")\n",
    "\n",
    "\n",
    "dates = [get_run_time(run) for run in bottom_runs]\n",
    "today = datetime.date.today()\n",
    "\n",
    "print(len(dates), np.shape(ch_df.loc[ch,:]), np.shape(ch_lerr_df.loc[ch, :]), np.shape(ch_uerr_df.loc[ch,:]))\n",
    "def plot_channel(ch):\n",
    "    f, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.fmt_xdata = mdates.DateFormatter('%d-%m')\n",
    "    ax.errorbar(dates, ch_df.loc[ch,:], yerr=[ch_lerr_df.loc[ch, :], ch_uerr_df.loc[ch,:]], linestyle='None', color='b', marker='.')\n",
    "    ax.xaxis.set_major_formatter(datetimefmt)\n",
    "    ax.xaxis.set_major_locator(months)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('SPE Acceptance')\n",
    "    plt.title('Channel %d' %ch)\n",
    "    f.autofmt_xdate()\n",
    "    plt.grid()\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "    plt.xlim(datetime.date(year=2018, month=1, day=1), datetime.date(year=2018, month=5, day=1) )\n",
    "    #plt.savefig('plots/ch%d.png' % ch)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table = \"^Channel^Acceptance^\\n\"\n",
    "\n",
    "for ch in [12, 34, 86, 134, 148, 190, 213]:\n",
    "    plot_channel(ch)\n",
    "    indices = [dates.index(date) for date in sorted(dates)[-2:]]\n",
    "    a = np.mean(ch_df.iloc[ch, indices])\n",
    "    table += \"|%d|%0.2f|\\n\" % (ch, a)\n",
    "\n",
    "table2 = \"^Channel^Acceptance^Uncertainty^\\n\"\n",
    "for ch in [27, 73, 167, 203]:\n",
    "    plot_channel(ch)\n",
    "    indices = [dates.index(date) for date in sorted(dates)[-2:]]\n",
    "    a = np.mean(ch_df.iloc[ch, indices])\n",
    "    table2 += \"|%d|%0.2f|\\n\" % (ch, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(table)\n",
    "\n",
    "print(table2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Further Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find outliers, plot the acceptance curve, acceptance histogram, and occupancy histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change the index in run=outliers[index] in order to change which outlier gets plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Find outliers, plot acc of one of them\n",
    "\n",
    "outlier_indices_low = np.array(np.where(accs < (evo_mean-1.5*evo_std))).flatten()\n",
    "outlier_indices_high= np.array(np.where(accs > (evo_mean+1.5*evo_std))).flatten()\n",
    "\n",
    "outlier_indices=outlier_indices_low.tolist() + outlier_indices_high.tolist()\n",
    "\n",
    "outliers = bottom_runs[outlier_indices]\n",
    "print(\"Outliers :\", outliers)\n",
    "run = outliers[0] #change index to change which outlier\n",
    "print('Run: ',run)\n",
    "runlist = file_to_list(find_file(run)[0])\n",
    "\n",
    "bot_run=runlist[0]\n",
    "tb_run=runlist[1]\n",
    "tr_run=runlist[2]\n",
    "\n",
    "x, acc, stat, sys = analyze.acceptance_curve_3runs(bot_run, tb_run, tr_run, val2corr2=6, space='amplitude')\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "for ch, a in enumerate(acc):\n",
    "    plt.plot(x, a, color='black', linewidth=1, alpha=0.6)\n",
    "    plt.title(\"Acceptance curve for: %d\" %run)\n",
    "    \n",
    "plt.xlim(-10, 200)\n",
    "plt.xlabel('threshold')\n",
    "plt.ylabel('spe acceptance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#doesn't save the data\n",
    "thresholds = analyze.get_thresholds(analyze.find_regular_run(bot_run))\n",
    "acc, acc_stat, acc_sys = analyze.acceptance_3runs(bot_run, tb_run, tr_run, thresholds, val2corr2=6, space='amplitude')\n",
    "occ, occ_sys, occ_stat = analyze.occupancy_3runs(bot_run, tb_run, tr_run, val2corr2=6, space='amplitude')\n",
    "\n",
    "on_channels = np.where(occ > 0.05)[0]\n",
    "\n",
    "acc=acc[on_channels]\n",
    "occ=occ[on_channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot the occupancy of one runlist\n",
    "plt.hist(occ, bins=50, range=(0,1.1))\n",
    "plt.title(\"Occupancy for %s \" %find_file(run)[0])\n",
    "plt.xlabel(\"Occupancy\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.xlim(0,0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot the acceptance of one runlist\n",
    "plt.hist(acc, bins=50, range=(0,1.1))\n",
    "plt.title(\"Acceptance for %s \" %find_file(run)[0])\n",
    "plt.xlabel(\"SPE Acceptance\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.xlim(0, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "runlist= find_file(run)[0]\n",
    "\n",
    "#plot acceptance of each channel\n",
    "plt.figure()\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.errorbar(on_channels, acc, yerr=[ch_lerr_df.loc[on_channels, runlist], ch_uerr_df.loc[on_channels,runlist]], marker='.', linestyle='none')\n",
    "plt.ylabel('Acceptance Fraction')\n",
    "plt.xlabel('Channel #')\n",
    "plt.title('Acc by Ch for %s' %runlist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cha_dict={}\n",
    "\n",
    "for ch, a in zip(on_channels, acc):\n",
    "    cha_dict[a]=ch\n",
    "    \n",
    "low_acc_ch=[]\n",
    "for a in acc:\n",
    "    if a<0.5:\n",
    "        low_acc_ch.append(cha_dict[a])\n",
    "        \n",
    "print('Low acc ch: ', low_acc_ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
