{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import analyze # takes some time since inits hax\n",
    "from channel_dict import channel_dict\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import pickle\n",
    "from scipy.stats import norm\n",
    "import shutil\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from make_runlist_new import write_spe_lists\n",
    "\n",
    "#dry run\n",
    "write_spe_lists(write=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#writes new runlists\n",
    "written=write_spe_lists(write=True)\n",
    "print(written)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This will take some time to run as job submission takes several hours. Alternatively, in the terminal you can run ./submit_jobs.sh ./runlists/[runlist name] for a single runlist, or ./large_submission.sh to submit jobs for all runlists in ./runlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def submit_job(file):\n",
    "    command = \"./submit_jobs.sh %s\" % file\n",
    "    print(command)\n",
    "    !{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in written:\n",
    "    submit_job(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acc Vs Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating acceptance and error data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import hax\n",
    "#hax already initiated when analyze is imported\n",
    "\n",
    "from spe_acceptance import data_dir_base, rawdata_dir\n",
    "\n",
    "\n",
    "def data_exists(run_number):\n",
    "    data_path=os.path.join(data_dir_base, 'run_%05d.h5' %int(run_number))\n",
    "    return os.path.exists(data_path)\n",
    "\n",
    "def all_data_exists(runlist):\n",
    "    return all([data_exists(run) for run in runlist])\n",
    "\n",
    "def file_to_list(runlist_file):\n",
    "    return [int(run) for run in runlist_file.split('.')[0].split('_')[1:4]]\n",
    "\n",
    "def get_run_time(run):\n",
    "    return hax.runs.datasets[hax.runs.datasets.number == run].start.values[0]\n",
    "    \n",
    "def find_file(run):\n",
    "    return [f for f in os.listdir('runlists') if any(int(run)==elem for elem in file_to_list(f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieves previously loaded data from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#all runlists\n",
    "runlists = [f for f in os.listdir('./runlists')]\n",
    "bottom_runs = []\n",
    "accs = []\n",
    "\n",
    "ch_acc_dict={}\n",
    "ch_err_l={}\n",
    "ch_err_u={}\n",
    "\n",
    "upper_errs=[]\n",
    "lower_errs=[]\n",
    "errors = []\n",
    "\n",
    "missing_runs = []\n",
    "LED_off = []\n",
    "resubmit_files = []\n",
    "\n",
    "with open('/project/lgrandi/xenon1t/spe_acceptance/ch_data/ch_data.pkl', 'rb') as cd:\n",
    "    if os.stat('/project/lgrandi/xenon1t/spe_acceptance/ch_data/ch_data.pkl').st_size==0:\n",
    "        print('No data in pickle file')\n",
    "        data=[]\n",
    "    else:\n",
    "        data=pickle.load(cd)\n",
    "saved_runlists=[]\n",
    "\n",
    "for runlist in runlists:\n",
    "    runs=file_to_list(runlist)\n",
    "    if not all_data_exists(runs):\n",
    "        resubmit_files.append(runlist)\n",
    "        for r in runs:\n",
    "            if not data_exists(r):\n",
    "                missing_runs.append(r)\n",
    "        continue\n",
    "    else:\n",
    "        bottom_runs.append(runs[0])\n",
    "\n",
    "for cd in data:\n",
    "    if len(data)==0:\n",
    "        continue\n",
    "        \n",
    "    saved_runlists.append(cd.runlist)\n",
    "    \n",
    "    acc=cd.acc\n",
    "    on_acc=cd.on_acc\n",
    "    acc_errs_l=cd.acc_errs_l\n",
    "    acc_errs_u=cd.acc_errs_u\n",
    "    acc_sys=cd.acc_sys\n",
    "    acc_stat=cd.acc_stat\n",
    "    occ=cd.occ\n",
    "    on_occ=cd.on_occ\n",
    "    occ_sys=cd.occ_sys\n",
    "    occ_stat=cd.occ_stat\n",
    "    on_channels = np.where(occ > 0.05)[0]\n",
    "        \n",
    "    if len(on_channels) < 200:\n",
    "        LED_off.append(cd.runlist)\n",
    "    \n",
    "    ch_acc_dict[cd.runlist]=acc\n",
    "    \n",
    "    #make dicts of runlist with errors for each channel\n",
    "    ch_err_l[cd.runlist]=acc_errs_l\n",
    "    ch_err_u[cd.runlist]=acc_errs_u\n",
    "    \n",
    "    runlist = file_to_list(cd.runlist)\n",
    "    \n",
    "    bottom_run = runlist[0]\n",
    "    topbulk_run = runlist[1]\n",
    "    topring_run = runlist[2]\n",
    "    \n",
    "    accs.append(np.mean(on_acc))\n",
    "    #mean to get average per runlist\n",
    "    lower_errs.append(np.mean(acc_errs_l**2))\n",
    "    upper_errs.append(np.mean(acc_errs_u**2))\n",
    "        \n",
    "print(\"Data already exists for: \", saved_runlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thresholds = analyze.get_thresholds(analyze.find_regular_run(10064))\n",
    "\n",
    "accs, acc_errs, sys_errs, stat_errs, t=analyze.acceptance_fraction(10064, thresholds[:248])\n",
    "print(t)\n",
    "print(*stat_errs[1, 200, 100:130])\n",
    "print(stat_errs[1, 200, t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def MC_errors(run_number,thresholds):\n",
    "        path = os.path.join(data_dir_base, 'run_%05d.h5' % run_number)\n",
    "        if not os.path.exists(path):\n",
    "            print(\"Acceptance data does not exist for run %d\" % run_number)\n",
    "        s=analyze.SPE(path)\n",
    "        #initialize error arrays\n",
    "        bin0 = np.where(s.data['bin_centers'] == 0.5)[0][0]\n",
    "        thresholds=np.array(thresholds[:248])\n",
    "        t=thresholds+bin0\n",
    "        sigma_l=np.zeros((248, len(s.data['bin_centers'])))\n",
    "        sigma_u=np.zeros((248, len(s.data['bin_centers'])))\n",
    "        ch_index = np.arange(248)\n",
    "        res, sigma_res=s.residual(6, 'amplitude')\n",
    "        #loop over channels\n",
    "        for ch in ch_index:\n",
    "            #make MC acc curves\n",
    "            acc_curves=s.acc_MC(res[ch], sigma_res[ch], 1000)\n",
    "            \n",
    "            acc_curves_t=acc_curves[ch, t[ch]]\n",
    "            #stats errors from MC\n",
    "            sigma_l[ch,:]=np.percentile(acc_curves[ch],16, axis=0)-np.mean(acc_curves[ch], axis=0)\n",
    "            sigma_u[ch,:]=np.percentile(acc_curves[ch], 84, axis=0)-np.mean(acc_curves[ch], axis=0)\n",
    "        print(*sigma_u[100,:])\n",
    "        stat_errs=np.array([sigma_l, sigma_u])\n",
    "       \n",
    "        return stat_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stat_errs=MC_errors(10064, thresholds)\n",
    "#print(*stat_errs[1, 200, 100:130])\n",
    "stat_errs[1, 100, t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculates acceptance, errors for new runlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#just newest runlist\n",
    "\n",
    "newest_runlist= find_file(sorted(bottom_runs)[-1])[0]\n",
    "print(\"Newest Runlist: \", newest_runlist)\n",
    "runlist = file_to_list(newest_runlist)\n",
    "\n",
    "if str(newest_runlist) in saved_runlists: #not in saved_runlists:\n",
    "    if not all_data_exists(runlist):\n",
    "        resubmit_files.append(newest_runlist)\n",
    "        for r in runlist:\n",
    "            if not data_exists(r):\n",
    "                missing_runs.append(r)\n",
    "            continue\n",
    "\n",
    "\n",
    "    bottom_run = runlist[0]\n",
    "    topbulk_run = runlist[1]\n",
    "    topring_run = runlist[2]\n",
    "\n",
    "    thresholds = analyze.get_thresholds(analyze.find_regular_run(bottom_run))\n",
    "\n",
    "    acc, acc_errs, acc_sys, acc_stat = analyze.acceptance_3runs(bottom_run, topbulk_run, topring_run, thresholds)\n",
    "    occ, occ_sys, occ_stat = analyze.occupancy_3runs(bottom_run, topbulk_run, topring_run)\n",
    "    print(acc_errs)\n",
    "\n",
    "    on_channels = np.where(occ > 0.05)[0]\n",
    "\n",
    "    if len(on_channels) < 200:\n",
    "        LED_off.append(newest_runlist)\n",
    "\n",
    "    on_acc = acc[on_channels]\n",
    "    on_occ = occ[on_channels]\n",
    "\n",
    "    ch_acc_dict[newest_runlist]=acc\n",
    "\n",
    "    acc_errs_l = acc_errs[0]\n",
    "    acc_errs_u = acc_errs[1]\n",
    "\n",
    "    ch_err_l[newest_runlist]=acc_errs_l\n",
    "    ch_err_u[newest_runlist]=acc_errs_u\n",
    "    print(acc_errs_l)\n",
    "\n",
    "    accs.append(np.mean(on_acc))\n",
    "    lower_errs.append(np.mean(acc_errs_l**2))\n",
    "    upper_errs.append(np.mean(acc_errs_u**2))\n",
    "    bottom_runs.append(bottom_run)\n",
    "\n",
    "    #cd=analyze.ch_data(newest_runlist, get_run_time(bottom_run), acc, on_acc, acc_errs_l, acc_errs_u, acc_sys, acc_stat, occ, on_occ, occ_sys, occ_stat)\n",
    "    #data.append(cd)\n",
    "\n",
    "else:\n",
    "    print(\"Data already in pickle file\")\n",
    "\n",
    "errors=np.array([lower_errs, upper_errs])\n",
    "print(lower_errs[-1])\n",
    "ch_err_l[newest_runlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(accs[-1])\n",
    "channels=np.arange(248)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.errorbar(channels, acc_df.loc[:, newest_runlist], yerr=[acc_errs_l, acc_errs_u], linestyle='None', marker='.' )\n",
    "#plt.ylim(0.7,1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#just newest runlist\n",
    "\n",
    "newest_runlist= find_file(sorted(bottom_runs)[-1])[0]\n",
    "print(\"Newest Runlist: \", newest_runlist)\n",
    "#runlist = file_to_list(newest_runlist)\n",
    "\n",
    "#if str(newest_runlist) not in saved_runlists:\n",
    "for runlist in tqdm(runlists):\n",
    "    runs=file_to_list(runlist)\n",
    "    if not all_data_exists(runs):\n",
    "        resubmit_files.append(runlist)\n",
    "        for r in runs:\n",
    "            if not data_exists(r):\n",
    "                missing_runs.append(r)\n",
    "        continue\n",
    "\n",
    "\n",
    "    bottom_run = runs[0]\n",
    "    topbulk_run = runs[1]\n",
    "    topring_run = runs[2]\n",
    "\n",
    "    thresholds = analyze.get_thresholds(analyze.find_regular_run(bottom_run))\n",
    "\n",
    "    acc, acc_errs, acc_sys, acc_stat = analyze.acceptance_3runs(bottom_run, topbulk_run, topring_run, thresholds)\n",
    "    occ, occ_sys, occ_stat = analyze.occupancy_3runs(bottom_run, topbulk_run, topring_run)\n",
    "\n",
    "    on_channels = np.where(occ > 0.05)[0]\n",
    "\n",
    "    if len(on_channels) < 200:\n",
    "        LED_off.append(runlist)\n",
    "\n",
    "    on_acc = acc[on_channels]\n",
    "    on_occ = occ[on_channels]\n",
    "\n",
    "    ch_acc_dict[runlist]=acc\n",
    "\n",
    "    acc_errs_l = acc_errs[0]\n",
    "    acc_errs_u = acc_errs[1]\n",
    "\n",
    "    ch_err_l[runlist]=np.mean(acc_errs_l**2, axis=1)\n",
    "    ch_err_u[runlist]=np.mean(acc_errs_u**2, axis=1)\n",
    "\n",
    "    accs.append(np.mean(on_acc))\n",
    "    lower_errs.append(np.mean(acc_errs_l**2))\n",
    "    upper_errs.append(np.mean(acc_errs_u**2))\n",
    "    bottom_runs.append(bottom_run)\n",
    "\n",
    "    cd=analyze.ch_data(runlist, get_run_time(bottom_run), acc, on_acc, acc_errs_l, acc_errs_u, acc_sys, acc_stat, occ, on_occ, occ_sys, occ_stat)\n",
    "    data.append(cd)\n",
    "\n",
    "#else:\n",
    " #   print(\"Data already in pickle file\")\n",
    "\n",
    "errors=np.array([lower_errs, upper_errs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dumps all data into pickle file, prints runs with LED off and runs with missing data, deletes raw data for processed runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from spe_acceptance import change_permissions\n",
    "\n",
    "os.remove('/project/lgrandi/xenon1t/spe_acceptance/ch_data/ch_data.pkl')\n",
    "with open('/project/lgrandi/xenon1t/spe_acceptance/ch_data/ch_data.pkl', 'wb') as cd:\n",
    "    pickle.dump(data, cd)\n",
    "change_permissions('/project/lgrandi/xenon1t/spe_acceptance/ch_data/ch_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Delete raw data after processed data is in the pickle file\n",
    "from get_name import get_name\n",
    "no_data=[]\n",
    "for runlist in runlists:\n",
    "    with open(os.path.join('./runlists', runlist)) as r:\n",
    "        for run in r.readlines():\n",
    "            name=get_name(int(run))\n",
    "            path=os.path.join(rawdata_dir, name)\n",
    "            if not os.path.exists(path):\n",
    "                no_data.append(run)\n",
    "                continue\n",
    "            if os.path.exists(path):\n",
    "                if os.path.exists(os.path.join(data_dir_base, run_%05d.h5 %run)):\n",
    "                    shutil.rmtree(path)\n",
    "                    print(\"Deleting raw data for: \", run, name)\n",
    "    \n",
    "print(\"These runs are missing data: \", missing_runs)\n",
    "print(\"LED likely off for these files: \", LED_off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resubmits jobs for runs missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in resubmit_files:\n",
    "    runs=file_to_list(f)\n",
    "    submit=os.path.join('./runlists', f)\n",
    "    if runs[0]<6731:\n",
    "        continue\n",
    "    submit_job(submit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acceptance dataframe with channels as row index, runlists as column index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "acc_df=pd.DataFrame(ch_acc_dict)\n",
    "channels_df=pd.DataFrame({\"channels\": [i for i in range(0,248)]})\n",
    "ch_df=pd.concat([acc_df, channels_df], axis=1)\n",
    "ch_df.set_index('channels', inplace=True)\n",
    "ch_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lower, upper errors dataframes with channels as row index, runlist as column index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lerr_df=pd.DataFrame(ch_err_l)\n",
    "ch_lerr_df=pd.concat([lerr_df, channels_df], axis=1)\n",
    "ch_lerr_df.set_index('channels', inplace=True)\n",
    "ch_lerr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "uerr_df=pd.DataFrame(ch_err_u)\n",
    "ch_uerr_df=pd.concat([uerr_df, channels_df], axis=1)\n",
    "ch_uerr_df.set_index('channels', inplace=True)\n",
    "ch_uerr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newest Runlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All channels for newest runlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc_df.loc[:, newest_runlist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Channels that look off, note that this includes both channels that are actually off and channels that aren't performing properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#prints list of off pmts\n",
    "my_off = np.where(occ < 0.05)[0]\n",
    "print(\"Channels that look off for %s: \" %newest_runlist , my_off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acceptance histogram for one runlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot the acceptance of one runlist\n",
    "plt.hist(on_acc, bins=50, range=(0,1.1))\n",
    "plt.title(\"Acceptance for %s \" %newest_runlist)\n",
    "plt.xlabel(\"SPE Acceptance\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.xlim(0, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Occupancy histogram for one runlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot the occupancy of one runlist\n",
    "plt.hist(on_occ, bins=50, range=(0,1.1))\n",
    "plt.title(\"Occupancy for %s \" %newest_runlist)\n",
    "plt.xlabel(\"Occupancy\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.xlim(0,0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acceptance of all channels for the newest runlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot acceptance of each channel\n",
    "plt.figure()\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.errorbar(on_channels, on_acc, yerr=[ch_lerr_df.loc[on_channels, newest_runlist], ch_uerr_df.loc[on_channels,newest_runlist]], marker='.', linestyle='none')\n",
    "plt.ylabel('Acceptance Fraction')\n",
    "plt.xlabel('Channel #')\n",
    "plt.title('Acc by Ch for %s' %newest_runlist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PMT Plot for Newest Runlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "analyze.plot_acceptances(acc_df.loc[:,newest_runlist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prints low acceptance channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#prints list of low acc ch\n",
    "cha_dict_new={}\n",
    "\n",
    "for ch, a in zip(on_channels, on_acc):\n",
    "    cha_dict_new[a]=ch\n",
    "    \n",
    "new_low_acc_ch=[]\n",
    "for a in on_acc:\n",
    "    if a<0.5:\n",
    "        new_low_acc_ch.append(cha_dict_new[a])\n",
    "        \n",
    "print('Low acc ch for %s: ' %newest_runlist, new_low_acc_ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean, Median, Errors for runlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#maybe including off channels in mean\n",
    "acc_mean=np.nanmean(ch_acc_dict[newest_runlist][on_channels])\n",
    "acc_median=np.nanmedian(ch_acc_dict[newest_runlist])\n",
    "\n",
    "print(\"Runlist: \", newest_runlist)\n",
    "print(\"Mean Acceptance: \", acc_mean)\n",
    "print(\"Median Acceptance: \", acc_median)\n",
    "print(\"Errors [l,u]: \", [np.nanmean(ch_err_l[newest_runlist]), np.nanmean(ch_err_u[newest_runlist])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Acceptance per Runlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot acceptance vs time using full runlists\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "years = mdates.YearLocator()   # every year\n",
    "months = mdates.MonthLocator()  # every month\n",
    "fmt = mdates.DateFormatter('%Y-%m')\n",
    "\n",
    "bottom_runs = np.array(bottom_runs)\n",
    "dates = np.array([get_run_time(run) for run in bottom_runs])\n",
    "accs = np.array(accs)\n",
    "\n",
    "noisy_runs = [12768, 13837]\n",
    "sr1 = np.where(bottom_runs > 6731)\n",
    "sr1 = [i for i in sr1[0] if bottom_runs[i] not in noisy_runs]\n",
    "\n",
    "plot_dates = dates[sr1]\n",
    "plot_runs = bottom_runs[sr1]\n",
    "plot_accs = accs[sr1]\n",
    "plot_errors = [errors[0][sr1],errors[1][sr1]]\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12,8))\n",
    "ax.errorbar(plot_dates, plot_accs, yerr=plot_errors, linestyle='None', marker='.')\n",
    "ax.set_ylim(0.75, 1.0)\n",
    "plt.xticks(ax.get_xticks(), ax.get_xticklabels(), rotation=20)\n",
    "ax.xaxis.set_major_locator(months)\n",
    "ax.xaxis.set_major_formatter(fmt)\n",
    "ax.grid()\n",
    "ax.set_xlabel('date')\n",
    "ax.set_ylabel('spe acceptance')\n",
    "plt.title(\"Acceptance vs Time\")\n",
    "plt.savefig('new_moneyplot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean, Median, Errors for all runlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#add table of mean/median w errors for time evo, print days that are outliers\n",
    "\n",
    "evo_mean=np.nanmean(plot_accs)\n",
    "evo_median=np.nanmedian(plot_accs)\n",
    "evo_errs=[np.nanmean(plot_errors[0]), np.nanmean(plot_errors[1])]\n",
    "\n",
    "evo_std=np.nanstd(plot_accs)\n",
    "\n",
    "print(\"All Runlists\")\n",
    "print(\"Mean :\", evo_mean)\n",
    "print(\"Median :\", evo_median)\n",
    "print(\"Errors [l, u]: \", evo_errs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acceptance for Individual Channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots acceptances of one channel for all runlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ch=19\n",
    "\n",
    "#list of lists of runlists\n",
    "sr1_rl=[find_file(i) for i in bottom_runs[sr1]]\n",
    "\n",
    "#list of runlists\n",
    "sr1_rl=[runlist for elem in sr1_rl for runlist in elem]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.errorbar(plot_dates, ch_df.loc[ch, sr1_rl] , yerr=[ch_lerr_df.loc[ch, sr1_rl], ch_uerr_df.loc[ch, sr1_rl]], linestyle='None', marker='.')\n",
    "plt.ylim(0.75, 1.1)\n",
    "plt.xticks(ax.get_xticks(), ax.get_xticklabels(), rotation=20)\n",
    "ax.xaxis.set_major_locator(months)\n",
    "ax.xaxis.set_major_formatter(fmt)\n",
    "plt.grid()\n",
    "plt.title(\"Channel %s\" %ch)\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('spe acceptance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for runlist in sr1_rl:\n",
    "    if ch_uerr_df.loc[ch, runlist]>0.05:\n",
    "        print(runlist, ch_uerr_df.loc[ch,runlist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prints runlists with low acceptance for one channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#prints list of low acc runlists\n",
    "\n",
    "ch_accs=np.array(ch_df.loc[ch])\n",
    "\n",
    "low_acc=np.where(ch_accs<0.5)\n",
    "\n",
    "low_acc_rl=list(ch_df.columns[low_acc])\n",
    "        \n",
    "print('Low acc runlists for ch %s: ' %ch, low_acc_rl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Further Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find outliers, plot the acceptance curve, acceptance histogram, and occupancy histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change the index in run=outliers[index] in order to change which outlier gets plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Find outliers, plot acc of one of them\n",
    "\n",
    "outlier_indices_low = np.array(np.where(accs < (evo_mean-1.5*evo_std))).flatten()\n",
    "outlier_indices_high= np.array(np.where(accs > (evo_mean+1.5*evo_std))).flatten()\n",
    "\n",
    "\n",
    "outlier_indices=outlier_indices_low.tolist() + outlier_indices_high.tolist()\n",
    "\n",
    "outliers = bottom_runs[outlier_indices]\n",
    "print(\"Outliers :\", outliers)\n",
    "run = outliers[0] #change index to change which outlier\n",
    "print('Run: ',run)\n",
    "runlist = file_to_list(find_file(run)[0])\n",
    "\n",
    "bot_run=runlist[0]\n",
    "tb_run=runlist[1]\n",
    "tr_run=runlist[2]\n",
    "\n",
    "x, acc, err = analyze.acceptance_curve_3runs(bot_run, tb_run, tr_run)\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "for ch, a in enumerate(acc):\n",
    "    plt.plot(x, a, color='black', linewidth=1, alpha=0.6)\n",
    "    plt.title(\"Acceptance curve for: %d\" %run)\n",
    "    \n",
    "plt.xlim(-10, 200)\n",
    "plt.xlabel('threshold')\n",
    "plt.ylabel('spe acceptance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#doesn't save the data\n",
    "thresholds = analyze.get_thresholds(analyze.find_regular_run(bot_run))\n",
    "acc, acc_errs, acc_sys, acc_stat = analyze.acceptance_3runs(bot_run, tb_run, tr_run, thresholds)\n",
    "occ, occ_sys, occ_stat = analyze.occupancy_3runs(bot_run, tb_run, tr_run)\n",
    "\n",
    "on_channels = np.where(occ > 0.05)[0]\n",
    "\n",
    "acc=acc[on_channels]\n",
    "occ=occ[on_channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot the occupancy of one runlist\n",
    "plt.hist(occ, bins=50, range=(0,1.1))\n",
    "plt.title(\"Occupancy for %s \" %find_file(run)[0])\n",
    "plt.xlabel(\"Occupancy\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.xlim(0,0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot the acceptance of one runlist\n",
    "plt.hist(acc, bins=50, range=(0,1.1))\n",
    "plt.title(\"Acceptance for %s \" %find_file(run)[0])\n",
    "plt.xlabel(\"SPE Acceptance\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.xlim(0, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "runlist= find_file(run)[0]\n",
    "\n",
    "#plot acceptance of each channel\n",
    "plt.figure()\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.errorbar(on_channels, acc, yerr=[ch_lerr_df.loc[on_channels, runlist], ch_uerr_df.loc[on_channels,runlist]], marker='.', linestyle='none')\n",
    "plt.ylabel('Acceptance Fraction')\n",
    "plt.xlabel('Channel #')\n",
    "plt.title('Acc by Ch for %s' %runlist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cha_dict={}\n",
    "\n",
    "for ch, a in zip(on_channels, acc):\n",
    "    cha_dict[a]=ch\n",
    "    \n",
    "low_acc_ch=[]\n",
    "for a in acc:\n",
    "    if a<0.5:\n",
    "        low_acc_ch.append(cha_dict[a])\n",
    "        \n",
    "print('Low acc ch: ', low_acc_ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
