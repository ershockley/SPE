{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import analyze # takes some time since inits hax\n",
    "from channel_dict import channel_dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#skipping runlist and submission stuff, should be done at this point\n",
    "\n",
    "ACTUALLY_OFF = [1, 2, 12, 26, 34, 62, 65, 79, 86, 88, 102, 118, \n",
    "                130, 134, 135, 139, 148, 150, 152, 162, 178, 183,\n",
    "                190, 198, 206, 213, 214, 234, 239, 244]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [00:39<03:04,  2.25s/it]/home/awalsh272/SPE/SPE/analyze.py:63: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return 1 - residual.cumsum(axis=1) / residual.sum(axis=1)[:, np.newaxis]\n",
      "/home/awalsh272/SPE/SPE/analyze.py:63: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return 1 - residual.cumsum(axis=1) / residual.sum(axis=1)[:, np.newaxis]\n",
      "/project/lgrandi/anaconda3/envs/pax_head/lib/python3.4/site-packages/numpy/core/_methods.py:112: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = asanyarray(arr - arrmean)\n",
      " 29%|██▉       | 29/100 [01:10<03:06,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LED likely OFF for runlist_13433_13434_13435.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [01:18<02:58,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LED likely OFF for runlist_14088_14089_14090.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [01:48<01:47,  2.35s/it]/home/awalsh272/SPE/SPE/analyze.py:31: RuntimeWarning: divide by zero encountered in log\n",
      "  occupancy_array[:,i] = -1*np.log(self.make_correction(val, 'amplitude'))\n",
      "100%|██████████| 100/100 [03:29<00:00,  2.59s/it]\n"
     ]
    }
   ],
   "source": [
    "#takes runlists and uses just bottom runs \n",
    "#finds acceptances and errors\n",
    "\n",
    "def data_exists(run_number):\n",
    "    return os.path.exists('./data/run_%05d.h5' % run_number)\n",
    "\n",
    "def all_data_exists(runlist):\n",
    "    return all([data_exists(run) for run in runlist])\n",
    "\n",
    "def file_to_list(runlist_file):\n",
    "    return [int(run) for run in runlist_file.split('.')[0].split('_')[1:4]]\n",
    "\n",
    "runlists = [f for f in os.listdir('./runlists')]\n",
    "\n",
    "bottom_runs = []\n",
    "accs = []\n",
    "errors = []\n",
    "\n",
    "missing_runs = []\n",
    "\n",
    "for f in tqdm(sorted(runlists)):\n",
    "    runlist = file_to_list(f)\n",
    "    if not all_data_exists(runlist):\n",
    "        #print('data missing for %s' % f)\n",
    "        for r in runlist:\n",
    "            if not data_exists(r):\n",
    "                missing_runs.append(r)\n",
    "        continue\n",
    "    bottom_run = runlist[0]\n",
    "    topbulk_run = runlist[1]\n",
    "    topring_run = runlist[2]\n",
    "    \n",
    "    thresholds = analyze.get_thresholds(analyze.find_regular_run(bottom_run))\n",
    "    \n",
    "    acc, acc_errs = analyze.acceptance_3runs(bottom_run, topbulk_run, topring_run, thresholds)\n",
    "    occ, occ_errs = analyze.occupancy_3runs(bottom_run, topbulk_run, topring_run)\n",
    "    \n",
    "    on_channels = np.where(occ > 0.05)[0]\n",
    "    \n",
    "    if len(on_channels) < 200:\n",
    "        print('LED likely OFF for %s' % f)\n",
    "        continue\n",
    "    \n",
    "    acc = acc[on_channels]\n",
    "    acc_errs = acc_errs[on_channels]\n",
    "    \n",
    "    accs.append(np.median(acc))\n",
    "    errors.append(np.sqrt((acc_errs**2).sum()) / len(on_channels))\n",
    "    bottom_runs.append(bottom_run)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check individual channel and run number using plot_channel\n",
    "\n",
    "ch='153' #user input\n",
    "runnum='14089' #user input\n",
    "\n",
    "#if you want to save the plots, add a path to filedir\n",
    "analyze.plot_channel(ch, runnum, xlims=(-100,500), ylims=(-100,500), filedir='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prints individual acceptance plots\n",
    "plots_path='/project/lgrandi/xenon1t/spe_acceptance/plots'\n",
    "\n",
    "run_number='14089' #user input, checks if plot already exists\n",
    "if os.path.exists(plots_path):\n",
    "    continue\n",
    "else:\n",
    "    analyze.plot_acceptances(accs, plots_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "means = []\n",
    "medians = []\n",
    "stds = []\n",
    "bot_runs = []\n",
    "\n",
    "n_bins = 40\n",
    "\n",
    "acc_array = np.ones((len(runlist_files), len(channel_dict['all_channels'])))\n",
    "\n",
    "\n",
    "for file, runlist in enumerate(sorted(runlist_files)):\n",
    "    runlist = runlist.split('/')[-1].split('_')\n",
    "    bottom_run = int(runlist[1])\n",
    "    topbulk_run = int(runlist[2])\n",
    "    topring_run = int(runlist[3][:-4])\n",
    "    csv_file = \"./acceptance_data/acceptances_%d_%d_%d.csv\" % (bottom_run, topbulk_run, topring_run)\n",
    "    \n",
    "    if os.path.exists(csv_file):\n",
    "        continue\n",
    "        \n",
    "        \n",
    "    existing_files = [os.path.exists('/home/ershockley/analysis/SPE/data/run_%d' % r)\n",
    "                        for r in [bottom_run, topbulk_run, topring_run]]\n",
    "    if not all([os.path.exists('/home/ershockley/analysis/SPE/data/run_%d' % r)\n",
    "                for r in [bottom_run, topbulk_run, topring_run]]):\n",
    "        missing_runs = [str(run) for run, boo in zip([bottom_run, topbulk_run, topring_run],\n",
    "                                               existing_files) if not boo]\n",
    "        print(\"Missing data for runs %s\" % \",\".join(missing_runs))\n",
    "        continue\n",
    "    \n",
    "    threshold_run = find_regular_run(bottom_run)\n",
    "    print(\"Threshold run: %d\" % threshold_run)\n",
    "    try:\n",
    "        thresholds = analyze.get_thresholds(threshold_run)\n",
    "    except KeyError:\n",
    "        thresholds = analyze.get_thresholds(threshold_run + 1)\n",
    "        \n",
    "    acceptances = analyze.get_acceptances_3runs(bottom_run, topring_run, \n",
    "                                                topbulk_run, thresholds, plot=True)\n",
    "    \n",
    "    \n",
    "    acc_array[file, :] *= acceptances\n",
    "    \n",
    "\n",
    "    with open(csv_file, \"w\") as f:\n",
    "        f.write(\"channel,acceptance\\n\")\n",
    "        for ch, acc in enumerate(acceptances):\n",
    "            f.write(\"%d,%0.4f\\n\" % (ch, acc))\n",
    "        \n",
    "    on_accs = acceptances\n",
    "    on_accs = np.delete(on_accs, analyze.excluded_pmts)\n",
    "    \n",
    "    means.append(np.mean(on_accs))\n",
    "    medians.append(np.median(on_accs))\n",
    "    stds.append(np.std(on_accs))\n",
    "    bot_runs.append(bottom_run)\n",
    "    plt.figure()\n",
    "    acc_hist, bins, patches = plt.hist(acceptances, bins = n_bins, range = (0,1))\n",
    "    plt.title(\"Runs %d %d %d\" % (bottom_run, topbulk_run, topring_run))\n",
    "    plt.xlabel(\"SPE acceptance fraction\")\n",
    "    plt.ylabel(\" # channels / bin (%d bins)\" % n_bins)\n",
    "    plt.show()\n",
    "    #plt.savefig(\"/project/lgrandi/xenon1t/spe_acceptance/plots/hist_%d-%d-%d.png\" % \n",
    "    #        (bottom_run, topbulk_run, topring_run))\n",
    "    \n",
    "print('DONE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
