{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import analyze # takes some time since inits hax\n",
    "from channel_dict import channel_dict\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import pickle\n",
    "from scipy.stats import norm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Job Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#job submission\n",
    "\n",
    "#dry run\n",
    "from make_runlist_new import write_spe_lists\n",
    "\n",
    "write_spe_lists(write=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#writes actual runlists\n",
    "\n",
    "written=write_spe_lists(write=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def submit_job(file):\n",
    "    command = \"./submit_jobs.sh %s\" % os.path.join('runlists', file)\n",
    "    print(command)\n",
    "    !{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#submit jobs for new runlists\n",
    "for f in written:\n",
    "    submit_job(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Acc vs Time Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ACTUALLY_OFF = [1, 2, 12, 26, 34, 62, 65, 79, 86, 88, 102, 118, \n",
    "                \n",
    "                130, 134, 135, 139, 148, 150, 152, 162, 178, 183,\n",
    "                190, 198, 206, 213, 214, 234, 239, 244]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import hax\n",
    "#hax already initiated when analyze is imported\n",
    "\n",
    "from spe_acceptance import data_dir_base\n",
    "\n",
    "\n",
    "def data_exists(run_number):\n",
    "    data_path=os.path.join(data_dir_base, 'run_%05d.h5' %run_number)\n",
    "    return os.path.exists(data_path)\n",
    "\n",
    "def all_data_exists(runlist):\n",
    "    return all([data_exists(run) for run in runlist])\n",
    "\n",
    "def file_to_list(runlist_file):\n",
    "    return [int(run) for run in runlist_file.split('.')[0].split('_')[1:4]]\n",
    "\n",
    "def get_run_time(run):\n",
    "    return hax.runs.datasets[hax.runs.datasets.number == run].start.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# acc for just two runlists\n",
    "\n",
    "#runlists = ['runlist_10610_10611_10612.txt','runlist_12211_12213_12214.txt','runlist_8840_8841_8842.txt']\n",
    "runlists = ['runlist_16984_16985_16986.txt']\n",
    "#runlists = ['runlist_17454_17455_17456.txt']\n",
    "#runlists = ['runlist_14733_14734_14735.txt']\n",
    "#runlists = ['runlist_17249_17250_17251.txt']\n",
    "#all runlists\n",
    "#runlists = [f for f in os.listdir('./runlists')]\n",
    "\n",
    "bottom_runs = []\n",
    "accs = []\n",
    "upper_errs=[]\n",
    "lower_errs=[]\n",
    "errors = []\n",
    "\n",
    "missing_runs = []\n",
    "LED_off = []\n",
    "resubmit_files = []\n",
    "\n",
    "with open('/project/lgrandi/xenon1t/spe_acceptance/ch_data/ch_data.pkl', 'rb') as cd:\n",
    "    if os.stat('/project/lgrandi/xenon1t/spe_acceptance/ch_data/ch_data.pkl').st_size==0:\n",
    "        data=[]\n",
    "    else:\n",
    "        data=pickle.load(cd)\n",
    "saved_runlists=[]\n",
    "\n",
    "for cd in data:\n",
    "    saved_runlists.append(cd.runlist)\n",
    "    \n",
    "    runlist = file_to_list(cd.runlist)\n",
    "            \n",
    "    bottom_run = runlist[0]\n",
    "    topbulk_run = runlist[1]\n",
    "    topring_run = runlist[2]\n",
    "    \n",
    "    acc=cd.acc\n",
    "    acc_errs_l=cd.acc_errs_l\n",
    "    acc_errs_u=cd.acc_errs_u\n",
    "    acc_sys=cd.acc_sys\n",
    "    acc_stat=cd.acc_stat\n",
    "    occ=cd.occ\n",
    "    occ_sys=cd.occ_sys\n",
    "    occ_stat=cd.occ_stat\n",
    "    \n",
    "    accs.append(np.mean(acc))\n",
    "    lower_errs.append(np.mean(acc_errs_l**2))\n",
    "    upper_errs.append(np.mean(acc_errs_u**2))\n",
    "    bottom_runs.append(bottom_run)\n",
    "            \n",
    "\n",
    "for f in tqdm(sorted(runlists)):\n",
    "    if f in saved_runlists:\n",
    "        print(\"Data already exists for: \", f)\n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "        runlist = file_to_list(f)\n",
    "        if not all_data_exists(runlist):\n",
    "            resubmit_files.append(f)\n",
    "            #print('data missing for %s' % f)\n",
    "            for r in runlist:\n",
    "                if not data_exists(r):\n",
    "                    missing_runs.append(r)\n",
    "            continue\n",
    "        \n",
    "    \n",
    "        bottom_run = runlist[0]\n",
    "        topbulk_run = runlist[1]\n",
    "        topring_run = runlist[2]\n",
    "        \n",
    "        thresholds = analyze.get_thresholds(analyze.find_regular_run(bottom_run))\n",
    "        \n",
    "        acc, acc_errs, acc_sys, acc_stat = analyze.acceptance_3runs(bottom_run, topbulk_run, topring_run, thresholds)\n",
    "        occ, occ_sys, occ_stat = analyze.occupancy_3runs(bottom_run, topbulk_run, topring_run)\n",
    "    \n",
    "        on_channels = np.where(occ > 0.05)[0]\n",
    "    \n",
    "        if len(on_channels) < 200:\n",
    "            LED_off.append(f)\n",
    "            #print('LED likely OFF for %s' % f)\n",
    "            continue\n",
    "        \n",
    "        acc = acc[on_channels]\n",
    "        acc_errs_l = acc_errs[0]\n",
    "        acc_errs_u = acc_errs[1]\n",
    "        accs.append(np.mean(acc))\n",
    "        lower_errs.append(np.mean(acc_errs_l**2, axis=0)[0])\n",
    "        upper_errs.append(np.mean(acc_errs_u**2, axis=0)[0])\n",
    "        bottom_runs.append(bottom_run)\n",
    "        cd=analyze.ch_data(f, get_run_time(bottom_run), acc, acc_errs_l, acc_errs_u, acc_sys, acc_stat, occ, occ_sys, occ_stat )\n",
    "        data.append(cd)\n",
    "\n",
    "os.remove('/project/lgrandi/xenon1t/spe_acceptance/ch_data/ch_data.pkl')\n",
    "with open('/project/lgrandi/xenon1t/spe_acceptance/ch_data/ch_data.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)\n",
    "    \n",
    "errors=np.array([lower_errs, upper_errs])\n",
    "print(errors)\n",
    "print(accs)\n",
    "print(\"These runs are missing data: \", missing_runs)\n",
    "print(\"LED likely off for these files: \", LED_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check that the MC is sampling correctly\n",
    "path = os.path.join(data_dir_base, 'run_%05d.h5' % 10610)\n",
    "s=analyze.SPE(path)\n",
    "residual, sigma_res= s.residual(6,'amplitude')\n",
    "mu=residual[176][106]\n",
    "sigma=sigma_res[176][106]\n",
    "print('mu: ',mu)\n",
    "print('sigma: ',sigma)\n",
    "x=np.linspace(mu-3*sigma, mu+3*sigma, num=1000)\n",
    "plt.figure()\n",
    "gauss=norm(loc=mu, scale=sigma)\n",
    "r=norm.rvs(loc=mu, scale=sigma, size=1000)\n",
    "plt.plot(x, gauss.pdf(x))\n",
    "plt.xlabel('Counts/Bin')\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.hist(r, color='r', normed=\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#get noise rms for acc vs time plot\n",
    "#explains outliers \n",
    "\n",
    "noise_runs = []\n",
    "noise_rms = []\n",
    "noise_errors = [[],[]]\n",
    "\n",
    "with open('./noise_rms.csv') as f:\n",
    "    for num, line in enumerate(f.readlines()):\n",
    "        if num==0:\n",
    "            continue\n",
    "        line = line.split(',')\n",
    "        run, rms, lower, upper = int(line[0]), float(line[1]), float(line[2]), float(line[3])\n",
    "        noise_runs.append(run)\n",
    "        noise_rms.append(rms)\n",
    "        noise_errors[0].append(lower)\n",
    "        noise_errors[1].append(upper)\n",
    "        \n",
    "noise_dates = [get_run_time(run) for run in noise_runs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#plot acceptance vs time \n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "years = mdates.YearLocator()   # every year\n",
    "months = mdates.MonthLocator()  # every month\n",
    "fmt = mdates.DateFormatter('%Y-%m')\n",
    "\n",
    "bottom_runs = np.array(bottom_runs)\n",
    "print('bottom_runs: ',bottom_runs)\n",
    "dates = np.array([get_run_time(run) for run in bottom_runs])\n",
    "\n",
    "accs = np.array(accs)\n",
    "errors = np.array(errors)\n",
    "print(errors)\n",
    "\n",
    "noisy_runs = [12768, 13837]\n",
    "\n",
    "sr1 = np.where(bottom_runs > 6731)\n",
    "print(sr1, sr1[0])\n",
    "sr1 = [i for i in sr1[0] if bottom_runs[i] not in noisy_runs]\n",
    "\n",
    "plot_dates = dates[sr1]\n",
    "plot_runs = bottom_runs[sr1]\n",
    "plot_accs = accs[sr1]\n",
    "plot_errors = [errors[0][sr1],errors[1][sr1]]\n",
    "print(plot_errors)\n",
    "print(accs)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12,8))\n",
    "#plt.errorbar(bottom_runs, accs, yerr=errors, linestyle='None', marker='.')\n",
    "ax.errorbar(plot_dates, plot_accs, yerr=plot_errors, linestyle='None', marker='.')\n",
    "ax.set_ylim(0.75, 1.0)\n",
    "plt.xticks(ax.get_xticks(), ax.get_xticklabels(), rotation=20)\n",
    "ax.xaxis.set_major_locator(months)\n",
    "ax.xaxis.set_major_formatter(fmt)\n",
    "ax.grid()\n",
    "ax.set_xlabel('data')\n",
    "ax.set_ylabel('spe acceptance')\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.scatter(noise_dates, noise_rms, color='red')\n",
    "ax2.tick_params('y', colors='red')\n",
    "ax2.set_ylabel('noise rms', color='red')\n",
    "ax2.set_ylim(1.8,3.8)\n",
    "plt.savefig('new_moneyplot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Resubmit jobs without all needed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#submits jobs that didn't work the first time\n",
    "for f in resubmit_files:\n",
    "    submit_job(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
